library(randomForest)
library(caret)
library(ggplot2)
library(randomForestExplainer)
?confusionMatrix
library(MASS)
library(ISLR2)
library(car)
head(Boston)
?Boston
summary(Boston$tax)
library(MASS)
library(ISLR2)
library(car)
###################################
##	Explore Dataset:
###################################
head(Boston)
#####
## Question #1 - What type of R data object is the Boston dataset, what are its dimensions,
##               and what variables are part of the dataset?
#####
###################################
##	Analysis: Simple Linear Regression
###################################
## Set up storage
lm.fit <- lm(medv ~ lstat)
lm.fit <- lm(medv ~ lstat, data = Boston)
attach(Boston)
lm.fit <- lm(medv ~ lstat)
# If we type the model name we get some information about the fit
lm.fit
# If we run summary of fit we get more detailed info about the fit
summary(lm.fit)
# Names gives us a list of all the info stored in the model fit
names(lm.fit)
# We can use extractor functions to access this info
coef(lm.fit)
confint(lm.fit)
#####
## Question #3 - How much of the variance in median home value is explained by this simple model?,
##               is there a significant relationship between these two variables (how do you know?),
##               what is the direction of the relationsip between these two variables?
#####
# Let's use the linear model to predict medv for specific values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")
# Now let's take a look at the model fit
plot(lstat, medv)
abline(lm.fit)
# We can play around with plotting lines and points
abline(lm.fit, lwd = 3)
abline(lm.fit, lwd = 3, col = "red")
plot(lstat, medv, col = "red")
plot(lstat, medv, pch = 20)
plot(lstat, medv, pch = "+")
plot(1:20, 1:20, pch = 1:20)
# Let's look at model diagnostics, this generates 4 plots so let's use par() to see them together
par(mfrow = c(2, 2))
plot(lm.fit)
# We can check out the residuals by plotting against the fitted values
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
# There definitely appears to be non-linearity so let's compute leverage stats.
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
# To fit a multiple regression model we simply add more predictors to the formula
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
# Let's try a model with all the variables to predict medv
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
# We now have a lot of variables in the mix so let's compute the VIF
vif(lm.fit)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
# Age is not a significant predictor by itself, but let's try an interaction of lstat and age
# The * syntax tells R to inlcude not only lstat and age separately, but also lstat multiplied by age
summary(lm(medv ~ lstat * age, data = Boston))
## Non-linear Transformations of the Predictors
# There was evidence of non-linearity so let's try including lstat squared
# Quadratic or polynomial terms must be inserted into the formula using I() as the ^ has a different meaning in formulas in R
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
# The inclusion of the quadratic term seems to improve the model
# Let's fit the basic model again (just with lstat) and then we can use an ANOVA to compare the two models
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
# Let's check out the diagnostic plots for the quadratic model and look at the residuals
par(mfrow = c(2, 2))
plot(lm.fit2)
# Let's try higher order polynomials in the fit
# We use poly() for convenience
lm.fit7 <- lm(medv ~ poly(lstat, 7))
summary(lm.fit7)
# Can also try other transforms of variables, e.g. log
summary(lm(medv ~ log(rm), data = Boston))
plot(lm(medv ~ log(rm), data = Boston))
plot(lm(medv ~ log(lstat), data = Boston))
?log
summary(lm.fit7)
plot(lm.fit2)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(medv ~ . - age, data = Boston)
plot(lm.fit1)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(log(medv) ~ . - age, data = Boston)
plot(lm.fit1)
?glm
glm.fit1 <- lm(medv ~ . - age, data = Boston, family = gamma)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = gamma)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = Gamma)
plot(glm.fit1)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = binomial)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = poisson)
plot(glm.fit1)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = quasibinomial)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = inverse.gaussian)
plot(glm.fit1)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = Gamma)
plot(glm.fit1)
plot(lm.fit1)
glm.fit1 <- glm(medv ~ ., data = Boston, family = Gamma)
plot(glm.fit1)
## Non-linear Transformations of the Predictors
# There was evidence of non-linearity so let's try including lstat squared
# Quadratic or polynomial terms must be inserted into the formula using I() as the ^ has a different meaning in formulas in R
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
plot(lm.fit2)
# Let's try higher order polynomials in the fit
# We use poly() for convenience
lm.fit7 <- lm(medv ~ poly(lstat, 7))
plot(lm.fit7)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(medv ~ . - age - indus, data = Boston)
summary(lm.fit1)
plot(lm.fit1)
sresid(studres(lm.fit1))
library(MASS)
sresid(studres(lm.fit1))
sresid <- studres(lm.fit1)
shapiro.test(sresid)
plot(lm.fit1, 4)
?plot
?lm
# Let's look at model diagnostics, this generates 4 plots so let's use par() to see them together
par(mfrow = c(3, 2))
plot(lm.fit1)
qqPlot(lm.fit1)
# Let's look at model diagnostics, this generates 4 plots so let's use par() to see them together
par(mfrow = c(2, 2))
qqPlot(lm.fit1)
dev.off()
qqPlot(lm.fit1)
lm.fit1
summary(lm.fit1)
confint(lm.fit1)
# Let's try a model with all the variables to predict medv
lm.fit <- lm(medv ~ ., data = Boston)
lm.fit
summary(lm.fit)
# We now have a lot of variables in the mix so let's compute the VIF
vif(lm.fit)
library(tree)
install.packages("tree")
library(tree)
library(ISLR2)
attach(Carseats)
?Carseats
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
Carseats <- data.frame(Carseats, High)
