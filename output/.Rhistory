library(randomForest)
library(caret)
library(ggplot2)
library(randomForestExplainer)
?confusionMatrix
library(MASS)
library(ISLR2)
library(car)
head(Boston)
?Boston
summary(Boston$tax)
library(MASS)
library(ISLR2)
library(car)
###################################
##	Explore Dataset:
###################################
head(Boston)
#####
## Question #1 - What type of R data object is the Boston dataset, what are its dimensions,
##               and what variables are part of the dataset?
#####
###################################
##	Analysis: Simple Linear Regression
###################################
## Set up storage
lm.fit <- lm(medv ~ lstat)
lm.fit <- lm(medv ~ lstat, data = Boston)
attach(Boston)
lm.fit <- lm(medv ~ lstat)
# If we type the model name we get some information about the fit
lm.fit
# If we run summary of fit we get more detailed info about the fit
summary(lm.fit)
# Names gives us a list of all the info stored in the model fit
names(lm.fit)
# We can use extractor functions to access this info
coef(lm.fit)
confint(lm.fit)
#####
## Question #3 - How much of the variance in median home value is explained by this simple model?,
##               is there a significant relationship between these two variables (how do you know?),
##               what is the direction of the relationsip between these two variables?
#####
# Let's use the linear model to predict medv for specific values of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")
# Now let's take a look at the model fit
plot(lstat, medv)
abline(lm.fit)
# We can play around with plotting lines and points
abline(lm.fit, lwd = 3)
abline(lm.fit, lwd = 3, col = "red")
plot(lstat, medv, col = "red")
plot(lstat, medv, pch = 20)
plot(lstat, medv, pch = "+")
plot(1:20, 1:20, pch = 1:20)
# Let's look at model diagnostics, this generates 4 plots so let's use par() to see them together
par(mfrow = c(2, 2))
plot(lm.fit)
# We can check out the residuals by plotting against the fitted values
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
# There definitely appears to be non-linearity so let's compute leverage stats.
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
# To fit a multiple regression model we simply add more predictors to the formula
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
# Let's try a model with all the variables to predict medv
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
# We now have a lot of variables in the mix so let's compute the VIF
vif(lm.fit)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
# Age is not a significant predictor by itself, but let's try an interaction of lstat and age
# The * syntax tells R to inlcude not only lstat and age separately, but also lstat multiplied by age
summary(lm(medv ~ lstat * age, data = Boston))
## Non-linear Transformations of the Predictors
# There was evidence of non-linearity so let's try including lstat squared
# Quadratic or polynomial terms must be inserted into the formula using I() as the ^ has a different meaning in formulas in R
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
# The inclusion of the quadratic term seems to improve the model
# Let's fit the basic model again (just with lstat) and then we can use an ANOVA to compare the two models
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
# Let's check out the diagnostic plots for the quadratic model and look at the residuals
par(mfrow = c(2, 2))
plot(lm.fit2)
# Let's try higher order polynomials in the fit
# We use poly() for convenience
lm.fit7 <- lm(medv ~ poly(lstat, 7))
summary(lm.fit7)
# Can also try other transforms of variables, e.g. log
summary(lm(medv ~ log(rm), data = Boston))
plot(lm(medv ~ log(rm), data = Boston))
plot(lm(medv ~ log(lstat), data = Boston))
?log
summary(lm.fit7)
plot(lm.fit2)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(medv ~ . - age, data = Boston)
plot(lm.fit1)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(log(medv) ~ . - age, data = Boston)
plot(lm.fit1)
?glm
glm.fit1 <- lm(medv ~ . - age, data = Boston, family = gamma)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = gamma)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = Gamma)
plot(glm.fit1)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = binomial)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = poisson)
plot(glm.fit1)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = quasibinomial)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = inverse.gaussian)
plot(glm.fit1)
glm.fit1 <- glm(medv ~ . - age, data = Boston, family = Gamma)
plot(glm.fit1)
plot(lm.fit1)
glm.fit1 <- glm(medv ~ ., data = Boston, family = Gamma)
plot(glm.fit1)
## Non-linear Transformations of the Predictors
# There was evidence of non-linearity so let's try including lstat squared
# Quadratic or polynomial terms must be inserted into the formula using I() as the ^ has a different meaning in formulas in R
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
plot(lm.fit2)
# Let's try higher order polynomials in the fit
# We use poly() for convenience
lm.fit7 <- lm(medv ~ poly(lstat, 7))
plot(lm.fit7)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
# Age appears to have a high p-value so let's re-run the model without it
lm.fit1 <- lm(medv ~ . - age - indus, data = Boston)
summary(lm.fit1)
plot(lm.fit1)
sresid(studres(lm.fit1))
library(MASS)
sresid(studres(lm.fit1))
sresid <- studres(lm.fit1)
shapiro.test(sresid)
plot(lm.fit1, 4)
?plot
?lm
# Let's look at model diagnostics, this generates 4 plots so let's use par() to see them together
par(mfrow = c(3, 2))
plot(lm.fit1)
qqPlot(lm.fit1)
# Let's look at model diagnostics, this generates 4 plots so let's use par() to see them together
par(mfrow = c(2, 2))
qqPlot(lm.fit1)
dev.off()
qqPlot(lm.fit1)
lm.fit1
summary(lm.fit1)
confint(lm.fit1)
# Let's try a model with all the variables to predict medv
lm.fit <- lm(medv ~ ., data = Boston)
lm.fit
summary(lm.fit)
# We now have a lot of variables in the mix so let's compute the VIF
vif(lm.fit)
library(tree)
install.packages("tree")
library(tree)
library(ISLR2)
attach(Carseats)
?Carseats
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
Carseats <- data.frame(Carseats, High)
library(sf)
library(terra)
library(dplyr)
library(future)
library(lgr)
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
library(mlr3extralearners)
library(mlr3spatiotempcv)
library(mlr3tuning)
library(mlr3viz)
library(progressr)
library(tmap)
library(spDataLarge)
# Set working directory, CHANGE TO YOUR PATH
setwd("/Volumes/KINGSTON UF/KINGSTON COPY/GeoAI/Labs/Lab 5 SVM for Landslide Susceptibility MASTER/output")
# Loading data from spDataLarge package
data("lsl", "study_mask", package = "spDataLarge")
ta = terra::rast(system.file("raster/ta.tif", package = "spDataLarge"))
task = mlr3spatiotempcv::TaskClassifST$new(
id = "ecuador_lsl",
backend = mlr3::as_data_backend(lsl), # backend expects that the input data includes the response and predictor variables
target = "lslpts", # target argument indicates the name of a response variable (in our case this is lslpts)
positive = "TRUE", # positive determines which of the two factor levels of the response variable indicate the landslide initiation point (in our case this is TRUE)
coordinate_names = c("x", "y"), # coordinate_names argument expects the names of the coordinate columns
coords_as_features = FALSE,
crs = "EPSG:32717" # need to indicate the used CRS (crs) and decide if we want to use the coordinates as predictors in the modeling (coords_as_features)
)
# Define the learner - can get list of learners here: https://mlr-org.com/learners.html
# We have a classification problem (landslide yes or no), let's use kernel SVM with the radial basis function
lrn_ksvm = mlr3::lrn("classif.ksvm", predict_type = "prob", kernel = "rbfdot", type = "C-svc")
install.packages("kernlab")
library(kernlab)
# Define the learner - can get list of learners here: https://mlr-org.com/learners.html
# We have a classification problem (landslide yes or no), let's use kernel SVM with the radial basis function
lrn_ksvm = mlr3::lrn("classif.ksvm", predict_type = "prob", kernel = "rbfdot", type = "C-svc")
# In case a model fails we provide a fallback learner, if the model fails to predict for an observation
# or predicts a missing value that prediction will be replaced with the most common classification
lrn_ksvm$fallback = lrn("classif.featureless", predict_type = "prob")
# Now we define the resampling method, i.e. spatial cv
resampling = mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100)
## Let's tune the hyperparameters of the SVM
# Make five spatially disjoint partitions, using the same data partitions for
# performance assessment and tuning would lead to overoptimistic results so must
# further partition the spatial folds
tune_level = mlr3::rsmp("spcv_coords", folds = 5)
ls()
?rsmp
